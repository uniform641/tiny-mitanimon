#!/usr/bin/env python3
"""
geofabrik_scrape.py

ä» Geofabrik download index é¡µé¢é€’å½’æŠ“å–æ¯ä¸ª subregion çš„:
 - name         (ä¾‹å¦‚ "Africa")
 - parent       (çˆ¶åç§°ï¼Œé¡¶å±‚ä¸ºç©ºå­—ç¬¦ä¸²)
 - size_bytes   (ä»¥å­—èŠ‚ä¸ºå•ä½ï¼Œè‹¥æœªçŸ¥ä¸º 0)
 - size_str     (åŸå§‹æ˜¾ç¤ºå­—ç¬¦ä¸²ï¼Œå¦‚ "(6.9 GB)")
 - download_link (æŒ‡å‘ <region>-latest.osm.pbf çš„å®Œæ•´ URL)

é€’å½’è§„åˆ™ï¼š
 - è‹¥è¯¥å­åŒºåŸŸçš„ .osm.pbf å¤§å° > 1 GiBï¼Œé€’å½’è¿›å…¥å¯¹åº”çš„å­é¡µé¢æŠ“å–å…¶å†…éƒ¨é¡¹
 - ç‰¹ä¾‹ï¼šé¡¶å±‚é¡µé¢çš„ "Central America" æ— è®ºå¤§å°ä¹Ÿä¼šè¢«é€’å½’æŠ“å–

ç”¨æ³•:
  python geofabrik_scrape.py [root_url]
é»˜è®¤ root_url = "https://download.geofabrik.de/index.html"
è¾“å‡º: geofabrik_regions.csv
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import csv
import re
import time

BASE_URL = "https://download.geofabrik.de/index.html"
OUTPUT_CSV = "geofabrik_regions.csv"

visited = set()

def parse_size(size_str: str) -> float:
    """å°† '6.9 GB' è½¬æˆå­—èŠ‚æ•°"""
    if not size_str:
        return 0
    s = size_str.strip().replace("(", "").replace(")", "").replace("\xa0", " ").upper()
    m = re.search(r"([\d.]+)\s*([KMG])B", s)
    if not m:
        return 0
    val, unit = m.groups()
    val = float(val)
    return val * {"K": 1024, "M": 1024**2, "G": 1024**3}[unit]

def get_soup(url: str) -> BeautifulSoup:
    r = requests.get(url)
    r.raise_for_status()
    return BeautifulSoup(r.text, "html.parser")

def parse_table(url: str, parent: str):
    print(f"ğŸ” Parsing {url}")
    soup = get_soup(url)

    tables = soup.find_all("table", id="subregions")
    valid_tables = [t for t in tables if t.find("tr", onmouseover=True)]
    if not valid_tables:
        return []

    records = []

    for table in valid_tables:
        for tr in table.find_all("tr", onmouseover=True):
            tds = tr.find_all("td")
            if len(tds) < 3:
                continue

            # ---- å­åŒºåŸŸåç§° ----
            name_tag = tds[0].find("a")
            if not name_tag:
                continue
            name = name_tag.text.strip()
            region_html = urljoin(url, name_tag["href"])

            # ---- ä¸‹è½½é“¾æ¥ ----
            pbf_tag = tds[1].find("a")
            download_link = urljoin(url, pbf_tag["href"]) if pbf_tag else ""

            # ---- æ–‡ä»¶å¤§å° ----
            size_text_raw = tds[2].get_text(strip=True)
            size_text = size_text_raw.replace("(", "").replace(")", "").replace("\xa0", " ").strip()
            size_bytes = parse_size(size_text)

            records.append({
                "name": name,
                "parent": parent,
                "size": size_text,
                "download_link": download_link
            })

            # ---- é€’å½’æ¡ä»¶ ----
            href_lower = name_tag["href"].lower()
            if (
                size_bytes > 1 * 1024**3 or  # å¤§äº1GB
                "central-america" in href_lower
            ):
                abs_url = urljoin(url, name_tag["href"])
                if abs_url not in visited:
                    visited.add(abs_url)
                    time.sleep(0.5)
                    records += parse_table(abs_url, parent=name)

    return records

def main():
    all_data = parse_table(BASE_URL, parent="root")

    with open(OUTPUT_CSV, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=["name", "parent", "size", "download_link"])
        writer.writeheader()
        writer.writerows(all_data)

    print(f"\nâœ… Done! Collected {len(all_data)} rows â†’ {OUTPUT_CSV}")

if __name__ == "__main__":
    main()
